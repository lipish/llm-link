# LLM Link Configuration - Codex CLI Standard OpenAI API
#
# This configuration provides a pure OpenAI-compatible API on standard port
# Designed specifically for Codex CLI and other OpenAI-compatible tools

# Server configuration - Standard HTTP port
server:
  host: "0.0.0.0"
  port: 8080  # Standard HTTP port (not Ollama's 11434)
  log_level: "info"

# LLM backend configuration
llm_backend:
  type: "Zhipu"
  base_url: "https://open.bigmodel.cn/api/paas/v4"
  api_key: "${ZHIPU_API_KEY}"
  model: "glm-4-flash"

# API endpoint configurations - Pure OpenAI API only
apis:
  # OpenAI-compatible API (PRIMARY and ONLY)
  openai:
    enabled: true
    path: "/v1"  # Standard OpenAI path - makes it http://localhost:8080/v1/
    api_key_header: "Authorization"
    api_key: "${LLM_LINK_API_KEY}"

  # Ollama API (DISABLED for pure OpenAI compatibility)
  ollama:
    enabled: false
    path: "/ollama"
    api_key_header: null
    api_key: null

  # Anthropic API (DISABLED)
  anthropic:
    enabled: false
    path: "/anthropic"

# Client adapter configuration
client_adapters:
  default_adapter: "standard"  # Pure OpenAI standard compliance
  
  # Disable Zed-specific adaptations for pure OpenAI compatibility
  zed:
    enabled: false
    force_images_field: false
    preferred_format: "json"
