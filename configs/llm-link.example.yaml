# LLM Link Configuration File
# This file configures the LLM proxy service

# Server configuration
server:
  host: "127.0.0.1"
  port: 8080
  log_level: "info"

# LLM backend configuration
# Choose ONE of the following backends:

# Option 1: OpenAI backend
llm_backend:
  type: "OpenAI"
  api_key: "sk-your-openai-api-key-here"
  base_url: "https://api.openai.com/v1"  # Optional, defaults to OpenAI official
  model: "gpt-3.5-turbo"

# Option 2: Anthropic backend
# llm_backend:
#   type: "Anthropic"
#   api_key: "sk-ant-your-anthropic-api-key-here"
#   model: "claude-3-sonnet-20240229"

# Option 3: Ollama backend (default - works with local Ollama installation)
# llm_backend:
#   type: "Ollama"
#   base_url: "http://localhost:11434"  # Optional, defaults to localhost:11434
#   model: "llama2"

# Option 4: Aliyun backend
# llm_backend:
#   type: "Aliyun"
#   api_key: "sk-your-aliyun-api-key-here"
#   model: "qwen-turbo"

# API endpoint configurations
# Enable/disable specific API interfaces
apis:
  # Ollama-compatible API
  ollama:
    enabled: true
    path: "/ollama"
    # This will be available at:
    # - POST /ollama/api/generate
    # - POST /ollama/api/chat
    # - GET /ollama/api/tags
    # - GET /ollama/api/show/:model

  # OpenAI-compatible API
  openai:
    enabled: true
    path: "/v1"
    api_key_header: null  # Optional custom header name for API key
    # This will be available at:
    # - POST /v1/chat/completions
    # - GET /v1/models

  # Anthropic-compatible API
  anthropic:
    enabled: true
    path: "/anthropic"
    api_key_header: null  # Optional custom header name for API key
    # This will be available at:
    # - POST /anthropic/v1/messages
    # - GET /anthropic/v1/models

# Example usage:
# 1. Copy this file to llm-link.yaml
# 2. Configure your desired backend and API keys
# 3. Start the service: `./target/release/llm-link -c llm-link.yaml`
# 4. Or use environment variables:
#    - LLM_LINK_CONFIG=./llm-link.yaml
#    - LLM_LINK_HOST=0.0.0.0
#    - LLM_LINK_PORT=8080
#    - LLM_LINK_LOG_LEVEL=debug