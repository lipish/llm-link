# LLM Link Configuration - OpenAI Codex CLI Compatible
#
# This configuration is optimized for OpenAI Codex CLI and other OpenAI-compatible tools
# It enables both OpenAI and Ollama APIs for maximum compatibility

# Server configuration
server:
  host: "0.0.0.0"  # Bind to all interfaces
  port: 11434      # Standard port (can be changed if needed)
  log_level: "info"

# LLM backend configuration - Using Zhipu GLM as backend
llm_backend:
  type: "Zhipu"
  base_url: "https://open.bigmodel.cn/api/paas/v4"
  api_key: "${ZHIPU_API_KEY}"  # Use environment variable for security
  model: "glm-4-flash"  # Fast model for code completion

# API endpoint configurations
apis:
  # OpenAI-compatible API (PRIMARY for Codex CLI)
  openai:
    enabled: true
    path: "/v1"  # Standard OpenAI API path
    # Optional: Add API key authentication if needed
    # api_key_header: "Authorization"
    # api_key: "Bearer your-api-key-here"

  # Ollama-compatible API (SECONDARY for other tools)
  ollama:
    enabled: true
    path: "/ollama"  # Non-conflicting path
    api_key_header: null
    api_key: null

  # Anthropic-compatible API (disabled)
  anthropic:
    enabled: false
    path: "/anthropic"

# Client adapter configuration
client_adapters:
  # Default to standard for OpenAI compatibility
  default_adapter: "standard"
  
  # Zed.dev specific configuration (if using both)
  zed:
    enabled: true
    force_images_field: true
    preferred_format: "ndjson"
