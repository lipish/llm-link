# LLM Link Configuration - OpenAI API on Standard Port
#
# This configuration runs OpenAI-compatible API on port 8080
# Suitable for tools that expect OpenAI API on a different port

# Server configuration
server:
  host: "0.0.0.0"
  port: 8080       # Standard web service port
  log_level: "info"

# LLM backend configuration
llm_backend:
  type: "Zhipu"
  base_url: "https://open.bigmodel.cn/api/paas/v4"
  api_key: "d2a0da2b02954b1f91a0a4ec16d4521b.GA2Tz9sF9kt4zVd3"
  model: "glm-4-flash"

# API endpoint configurations
apis:
  # OpenAI-compatible API (PRIMARY)
  openai:
    enabled: true
    path: "/v1"  # Standard OpenAI path: http://localhost:8080/v1/

  # Ollama-compatible API (disabled to avoid conflicts)
  ollama:
    enabled: false
    path: "/ollama"

  # Anthropic-compatible API (disabled)
  anthropic:
    enabled: false
    path: "/anthropic"

# Client adapter configuration
client_adapters:
  default_adapter: "standard"  # Pure OpenAI compatibility
