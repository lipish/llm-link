use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Config {
    pub server: ServerConfig,
    pub llm_backend: LlmBackendConfig,
    pub apis: ApiConfigs,
    pub client_adapters: Option<ClientAdapterConfigs>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ServerConfig {
    pub host: String,
    pub port: u16,
    pub log_level: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type")]
pub enum LlmBackendConfig {
    OpenAI {
        api_key: String,
        base_url: Option<String>,
        model: String,
    },
    Anthropic {
        api_key: String,
        model: String,
    },
    Ollama {
        base_url: Option<String>,
        model: String,
    },
    Zhipu {
        api_key: String,
        base_url: Option<String>,
        model: String,
    },
    Aliyun {
        api_key: String,
        model: String,
    },
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ApiConfigs {
    pub ollama: Option<OllamaApiConfig>,
    pub openai: Option<OpenAiApiConfig>,
    pub anthropic: Option<AnthropicApiConfig>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ClientAdapterConfigs {
    /// 默认客户端适配模式
    pub default_adapter: Option<String>,
    /// 强制客户端适配模式（忽略自动检测）
    pub force_adapter: Option<String>,
    /// Zed.dev 特定配置
    pub zed: Option<ZedAdapterConfig>,
    /// Zhipu 特定配置
    pub zhipu: Option<ZhipuAdapterConfig>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ZedAdapterConfig {
    /// 是否启用 Zed.dev 适配
    pub enabled: bool,
    /// 是否强制添加 images 字段
    pub force_images_field: Option<bool>,
    /// 首选响应格式
    pub preferred_format: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ZhipuAdapterConfig {
    /// 是否启用 XML 到 JSON 转换（针对标准客户端如 Codex/Zed）
    /// 默认为 true，即自动转换 XML function calls 为 JSON 格式
    pub convert_xml_to_json: Option<bool>,
    /// 是否保留原始 XML 格式（针对 Zhipu 原生应用）
    /// 默认为 false
    pub preserve_xml: Option<bool>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OllamaApiConfig {
    pub enabled: bool,
    pub path: String,
    pub api_key_header: Option<String>,
    pub api_key: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAiApiConfig {
    pub enabled: bool,
    pub path: String,
    pub api_key_header: Option<String>,
    pub api_key: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnthropicApiConfig {
    pub enabled: bool,
    pub path: String,
    pub api_key_header: Option<String>,
}

impl Default for Config {
    fn default() -> Self {
        Self {
            server: ServerConfig {
                host: "127.0.0.1".to_string(),
                port: 8080,
                log_level: "info".to_string(),
            },
            llm_backend: LlmBackendConfig::Ollama {
                base_url: Some("http://localhost:11434".to_string()),
                model: "llama2".to_string(),
            },
            apis: ApiConfigs {
                ollama: Some(OllamaApiConfig {
                    enabled: true,
                    path: "/ollama".to_string(),
                    api_key_header: None,
                    api_key: None,
                }),
                openai: Some(OpenAiApiConfig {
                    enabled: true,
                    path: "/v1".to_string(),
                    api_key_header: None,
                    api_key: None,
                }),
                anthropic: Some(AnthropicApiConfig {
                    enabled: true,
                    path: "/anthropic".to_string(),
                    api_key_header: None,
                }),
            },
            client_adapters: None,
        }
    }
}

impl Config {
    // Configuration is now generated by AppConfigGenerator only
    // No file-based configuration loading needed
}

