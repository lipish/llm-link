use axum::{
    extract::{Query, State},
    http::{HeaderMap, StatusCode},
    response::{IntoResponse, Json},
    response::Response,
    body::Body,
};
use futures::StreamExt;
use serde::{Deserialize, Serialize};
use serde_json::{json, Value};
use std::convert::Infallible;
use tracing::{info, warn, error};

use crate::adapters::{ClientAdapter, FormatDetector};
use crate::handlers::AppState;
use crate::service;

#[derive(Debug, Deserialize)]
pub struct OllamaChatRequest {
    pub model: String,
    pub messages: Vec<Value>,
    pub stream: Option<bool>,
    pub options: Option<Value>,
}

#[derive(Debug, Deserialize)]
pub struct OllamaTagsParams {
    // Ollama tags endpoint parameters (if any)
}

/// Ollama Chat API
pub async fn chat(
    headers: HeaderMap,
    State(state): State<AppState>,
    Json(request): Json<OllamaChatRequest>,
) -> Result<Response, StatusCode> {
    // Ollama API é€šå¸¸ä¸éœ€è¦è®¤è¯ï¼Œä½†å¯ä»¥é…ç½®
    if let Some(cfg) = &state.config.apis.ollama {
        if let Some(expected_key) = cfg.api_key.as_ref() {
            // å¦‚æœé…ç½®äº† API keyï¼Œåˆ™è¿›è¡ŒéªŒè¯
            // è¿™é‡Œå¯ä»¥æ·»åŠ  Ollama API key éªŒè¯é€»è¾‘
        }
    }
    
    // éªŒè¯æ¨¡å‹
    if !request.model.is_empty() {
        match state.llm_service.validate_model(&request.model).await {
            Ok(false) => return Err(StatusCode::BAD_REQUEST),
            Err(_) => return Err(StatusCode::INTERNAL_SERVER_ERROR),
            Ok(true) => {}
        }
    }

    // è½¬æ¢æ¶ˆæ¯æ ¼å¼
    match service::convert_openai_messages(request.messages) {
        Ok(messages) => {
            let model = if request.model.is_empty() { None } else { Some(request.model.as_str()) };

            if request.stream.unwrap_or(false) {
                handle_streaming_request(headers, state, model, messages).await
            } else {
                handle_non_streaming_request(state, model, messages).await
            }
        }
        Err(_) => Err(StatusCode::BAD_REQUEST),
    }
}

/// å¤„ç†æµå¼è¯·æ±‚
async fn handle_streaming_request(
    headers: HeaderMap,
    state: AppState,
    model: Option<&str>,
    messages: Vec<crate::client::Message>,
) -> Result<Response, StatusCode> {
    // ğŸ¯ æ£€æµ‹å®¢æˆ·ç«¯ç±»å‹ï¼ˆZed.dev æˆ–æ ‡å‡†ï¼‰
    let client_adapter = detect_ollama_client(&headers, &state.config);
    let (stream_format, _) = FormatDetector::determine_format(&headers);
    
    // ä½¿ç”¨æ£€æµ‹åˆ°çš„æ ¼å¼æˆ–å®¢æˆ·ç«¯åå¥½
    let final_format = if headers.get("accept").map_or(true, |v| v.to_str().unwrap_or("").contains("*/*")) {
        client_adapter.preferred_format()
    } else {
        stream_format
    };
    
    let content_type = FormatDetector::get_content_type(final_format);

    info!("ğŸ“¡ Starting Ollama streaming response - Client: {:?}, Format: {:?} ({})",
          client_adapter, final_format, content_type);

    match state.llm_service.chat_stream_with_format(model, messages.clone(), final_format).await {
        Ok(rx) => {
            info!("âœ… Ollama streaming response started successfully");

            let config = state.config.clone();
            let adapted_stream = rx.map(move |data| {
                // è§£æå¹¶é€‚é…å“åº”æ•°æ®
                if let Ok(mut json_data) = serde_json::from_str::<Value>(&data) {
                    client_adapter.apply_response_adaptations(&config, &mut json_data);

                    match final_format {
                        llm_connector::StreamFormat::SSE => {
                            format!("data: {}\n\n", json_data)
                        }
                        llm_connector::StreamFormat::NDJSON => {
                            format!("{}\n", json_data)
                        }
                        llm_connector::StreamFormat::Json => {
                            json_data.to_string()
                        }
                    }
                } else {
                    data.to_string()
                }
            });

            let body_stream = adapted_stream.map(|data| Ok::<_, Infallible>(data));
            let body = Body::from_stream(body_stream);

            let response = Response::builder()
                .status(200)
                .header("content-type", content_type)
                .header("cache-control", "no-cache")
                .body(body)
                .unwrap();

            Ok(response)
        }
        Err(e) => {
            warn!("âš ï¸ Ollama streaming failed, falling back to non-streaming: {:?}", e);
            handle_non_streaming_request(state, model, messages).await
        }
    }
}

/// å¤„ç†éæµå¼è¯·æ±‚
async fn handle_non_streaming_request(
    state: AppState,
    model: Option<&str>,
    messages: Vec<crate::client::Message>,
) -> Result<Response, StatusCode> {
    match state.llm_service.chat_with_model(model, messages).await {
        Ok(response) => {
            let ollama_response = service::convert_response_to_ollama(response);
            Ok(Json(ollama_response).into_response())
        }
        Err(e) => {
            error!("âŒ Ollama chat request failed: {:?}", e);
            Err(StatusCode::INTERNAL_SERVER_ERROR)
        }
    }
}

/// Ollama Tags API
pub async fn tags(
    _headers: HeaderMap,
    State(state): State<AppState>,
    Query(_params): Query<OllamaTagsParams>,
) -> Result<impl IntoResponse, StatusCode> {
    match state.llm_service.models().await {
        Ok(models) => {
            let response = json!({
                "models": models
            });
            Ok(Json(response))
        }
        Err(_) => Err(StatusCode::INTERNAL_SERVER_ERROR),
    }
}

/// æ£€æµ‹ Ollama å®¢æˆ·ç«¯ç±»å‹
fn detect_ollama_client(headers: &HeaderMap, config: &crate::config::Config) -> ClientAdapter {
    // 1. æ£€æŸ¥å¼ºåˆ¶é€‚é…å™¨è®¾ç½®
    if let Some(ref adapters) = config.client_adapters {
        if let Some(force_adapter) = &adapters.force_adapter {
            match force_adapter.to_lowercase().as_str() {
                "zed" | "zed.dev" => return ClientAdapter::ZedDev,
                "standard" => return ClientAdapter::Standard,
                _ => {}
            }
        }
    }

    // 2. æ£€æŸ¥æ˜¾å¼å®¢æˆ·ç«¯æ ‡è¯†
    if let Some(client) = headers.get("x-client") {
        if let Ok(client_str) = client.to_str() {
            match client_str.to_lowercase().as_str() {
                "zed" | "zed.dev" => return ClientAdapter::ZedDev,
                "standard" => return ClientAdapter::Standard,
                _ => {}
            }
        }
    }

    // 3. æ£€æŸ¥ User-Agent è‡ªåŠ¨æ£€æµ‹
    if let Some(user_agent) = headers.get("user-agent") {
        if let Ok(ua_str) = user_agent.to_str() {
            // æ£€æµ‹ Zed.dev ç¼–è¾‘å™¨
            if ua_str.starts_with("Zed/") {
                if let Some(ref adapters) = config.client_adapters {
                    if let Some(ref zed_config) = adapters.zed {
                        if zed_config.enabled {
                            return ClientAdapter::ZedDev;
                        }
                    }
                }
            }
        }
    }

    // 4. ä½¿ç”¨é»˜è®¤é€‚é…å™¨
    if let Some(ref adapters) = config.client_adapters {
        if let Some(default_adapter) = &adapters.default_adapter {
            match default_adapter.to_lowercase().as_str() {
                "zed" | "zed.dev" => return ClientAdapter::ZedDev,
                "standard" => return ClientAdapter::Standard,
                _ => {}
            }
        }
    }

    // 5. æœ€ç»ˆé»˜è®¤
    ClientAdapter::Standard
}
