use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Settings {
    pub server: ServerSettings,
    pub llm_backend: LlmBackendSettings,
    pub apis: ApiSettings,
    pub client_adapters: Option<ClientAdapterSettings>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ServerSettings {
    pub host: String,
    pub port: u16,
    pub log_level: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type")]
pub enum LlmBackendSettings {
    OpenAI {
        api_key: String,
        base_url: Option<String>,
        model: String,
    },
    Anthropic {
        api_key: String,
        model: String,
    },
    Ollama {
        base_url: Option<String>,
        model: String,
    },
    Zhipu {
        api_key: String,
        base_url: Option<String>,
        model: String,
    },
    Aliyun {
        api_key: String,
        model: String,
    },
    Volcengine {
        api_key: String,
        model: String,
    },
    Tencent {
        api_key: String,
        model: String,
    },
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ApiSettings {
    pub ollama: Option<OllamaApiSettings>,
    pub openai: Option<OpenAiApiSettings>,
    pub anthropic: Option<AnthropicApiSettings>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ClientAdapterSettings {
    /// 默认客户端适配模式
    pub default_adapter: Option<String>,
    /// 强制客户端适配模式（忽略自动检测）
    pub force_adapter: Option<String>,
    /// Zed.dev 特定配置
    pub zed: Option<ZedAdapterSettings>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ZedAdapterSettings {
    /// 是否启用 Zed.dev 适配
    pub enabled: bool,
    /// 是否强制添加 images 字段
    pub force_images_field: Option<bool>,
    /// 首选响应格式
    pub preferred_format: Option<String>,
}



#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OllamaApiSettings {
    pub enabled: bool,
    pub path: String,
    pub api_key_header: Option<String>,
    pub api_key: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAiApiSettings {
    pub enabled: bool,
    pub path: String,
    pub api_key_header: Option<String>,
    pub api_key: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnthropicApiSettings {
    pub enabled: bool,
    pub path: String,
    pub api_key_header: Option<String>,
}

impl Default for Settings {
    fn default() -> Self {
        Self {
            server: ServerSettings {
                host: "127.0.0.1".to_string(),
                port: 8080,
                log_level: "info".to_string(),
            },
            llm_backend: LlmBackendSettings::Ollama {
                base_url: Some("http://localhost:11434".to_string()),
                model: "llama2".to_string(),
            },
            apis: ApiSettings {
                ollama: Some(OllamaApiSettings {
                    enabled: true,
                    path: "/ollama".to_string(),
                    api_key_header: None,
                    api_key: None,
                }),
                openai: Some(OpenAiApiSettings {
                    enabled: true,
                    path: "/v1".to_string(),
                    api_key_header: None,
                    api_key: None,
                }),
                anthropic: Some(AnthropicApiSettings {
                    enabled: true,
                    path: "/anthropic".to_string(),
                    api_key_header: None,
                }),
            },
            client_adapters: None,
        }
    }
}

impl Settings {
    // Settings are now generated by AppConfigGenerator only
    // No file-based configuration loading needed
}

