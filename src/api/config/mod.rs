use axum::{extract::State, http::StatusCode, response::Json};
use serde::{Deserialize, Serialize};
use serde_json::json;
use tracing::{info, error};
use std::sync::atomic::{AtomicU64, Ordering};

use crate::api::AppState;

/// å®‰å…¨åœ°æ©ç›– API Key ç”¨äºæ—¥å¿—è®°å½•
fn mask_api_key(api_key: &str) -> String {
    if api_key.len() <= 8 {
        "*".repeat(api_key.len())
    } else {
        format!("{}***{}", &api_key[..4], &api_key[api_key.len()-4..])
    }
}

/// éªŒè¯ API Key æ ¼å¼
fn validate_api_key(provider: &str, api_key: &str) -> Result<(), String> {
    if api_key.trim().is_empty() {
        return Err("API key cannot be empty".to_string());
    }

    match provider {
        "openai" => {
            if !api_key.starts_with("sk-") {
                return Err("OpenAI API key should start with 'sk-'".to_string());
            }
        }
        "anthropic" => {
            if !api_key.starts_with("sk-ant-") {
                return Err("Anthropic API key should start with 'sk-ant-'".to_string());
            }
        }
        "zhipu" => {
            // Zhipu API keys have specific format, but we'll be lenient
            if api_key.len() < 10 {
                return Err("Zhipu API key seems too short".to_string());
            }
        }
        "ollama" => {
            // Ollama doesn't require API key, so this is always valid
        }
        _ => {
            // For other providers, just check minimum length
            if api_key.len() < 10 {
                return Err("API key seems too short".to_string());
            }
        }
    }

    Ok(())
}

/// éªŒè¯ provider åç§°
fn validate_provider(provider: &str) -> Result<(), String> {
    match provider {
        "openai" | "anthropic" | "zhipu" | "ollama" | "aliyun" | "volcengine" | "tencent" => Ok(()),
        _ => Err(format!("Unsupported provider: {}", provider)),
    }
}

// å…¨å±€è®¡æ•°å™¨ï¼Œæ¯æ¬¡å¯åŠ¨æ—¶é€’å¢
static INSTANCE_ID: AtomicU64 = AtomicU64::new(0);

pub fn init_instance_id() {
    use std::time::{SystemTime, UNIX_EPOCH};
    let timestamp = SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .unwrap()
        .as_secs();
    INSTANCE_ID.store(timestamp, Ordering::SeqCst);
}

pub fn get_instance_id() -> u64 {
    INSTANCE_ID.load(Ordering::SeqCst)
}

#[derive(Debug, Deserialize)]
pub struct UpdateConfigRequest {
    pub provider: String,
    pub api_key: String,
    #[serde(default)]
    pub model: Option<String>,
    #[serde(default)]
    pub base_url: Option<String>,
}

#[derive(Debug, Deserialize)]
pub struct UpdateKeyRequest {
    pub provider: String,
    pub api_key: String,
    #[serde(default)]
    pub base_url: Option<String>,
}

#[derive(Debug, Deserialize)]
pub struct SwitchProviderRequest {
    pub provider: String,
    #[serde(default)]
    pub model: Option<String>,
    #[serde(default)]
    pub api_key: Option<String>,
    #[serde(default)]
    pub base_url: Option<String>,
}

#[derive(Debug, Serialize)]
pub struct CurrentConfigResponse {
    pub provider: String,
    pub model: String,
    pub has_api_key: bool,
    pub has_base_url: bool,
    pub supports_hot_reload: bool,
}

/// è·å–å½“å‰é…ç½®ä¿¡æ¯ï¼ˆä¸åŒ…å«æ•æ„Ÿçš„ API Keyï¼‰
pub async fn get_current_config(
    State(state): State<AppState>,
) -> Result<Json<CurrentConfigResponse>, StatusCode> {
    use crate::settings::LlmBackendSettings;

    let config = state.config.read().unwrap();
    let (provider, model, has_api_key, has_base_url) = match &config.llm_backend {
        LlmBackendSettings::OpenAI { model, base_url, .. } => {
            ("openai", model.clone(), true, base_url.is_some())
        }
        LlmBackendSettings::Anthropic { model, .. } => {
            ("anthropic", model.clone(), true, false)
        }
        LlmBackendSettings::Zhipu { model, base_url, .. } => {
            ("zhipu", model.clone(), true, base_url.is_some())
        }
        LlmBackendSettings::Ollama { model, base_url } => {
            ("ollama", model.clone(), false, base_url.is_some())
        }
        LlmBackendSettings::Aliyun { model, .. } => {
            ("aliyun", model.clone(), true, false)
        }
        LlmBackendSettings::Volcengine { model, .. } => {
            ("volcengine", model.clone(), true, false)
        }
        LlmBackendSettings::Tencent { model, .. } => {
            ("tencent", model.clone(), true, false)
        }
    };
    
    Ok(Json(CurrentConfigResponse {
        provider: provider.to_string(),
        model,
        has_api_key,
        has_base_url,
        supports_hot_reload: true, // ç°åœ¨æ”¯æŒçƒ­é‡è½½
    }))
}

/// è·å–å¥åº·çŠ¶æ€å’Œå®ä¾‹ä¿¡æ¯
///
/// ç”¨äºéªŒè¯æœåŠ¡æ˜¯å¦é‡å¯æˆåŠŸ
pub async fn get_health(
    State(state): State<AppState>,
) -> Json<serde_json::Value> {
    use crate::settings::LlmBackendSettings;

    let config = state.config.read().unwrap();
    let (provider, model) = match &config.llm_backend {
        LlmBackendSettings::OpenAI { model, .. } => ("openai", model.clone()),
        LlmBackendSettings::Anthropic { model, .. } => ("anthropic", model.clone()),
        LlmBackendSettings::Zhipu { model, .. } => ("zhipu", model.clone()),
        LlmBackendSettings::Ollama { model, .. } => ("ollama", model.clone()),
        LlmBackendSettings::Aliyun { model, .. } => ("aliyun", model.clone()),
        LlmBackendSettings::Volcengine { model, .. } => ("volcengine", model.clone()),
        LlmBackendSettings::Tencent { model, .. } => ("tencent", model.clone()),
    };
    
    Json(json!({
        "status": "ok",
        "instance_id": get_instance_id(),
        "pid": std::process::id(),
        "provider": provider,
        "model": model,
    }))
}

/// æ›´æ–°é…ç½®å¹¶è¯·æ±‚é‡å¯
/// 
/// è¿™ä¸ªç«¯ç‚¹ä¼š:
/// 1. éªŒè¯é…ç½®çš„æœ‰æ•ˆæ€§
/// 2. å°†é…ç½®ä¿å­˜ä¸ºç¯å¢ƒå˜é‡æ ¼å¼ï¼ˆä¾›è°ƒç”¨è€…é‡å¯æ—¶ä½¿ç”¨ï¼‰
/// 3. è¿”å›éœ€è¦è®¾ç½®çš„ç¯å¢ƒå˜é‡
/// 
/// z-agent éœ€è¦:
/// 1. è°ƒç”¨æ­¤ç«¯ç‚¹è·å–ç¯å¢ƒå˜é‡
/// 2. ä½¿ç”¨æ–°çš„ç¯å¢ƒå˜é‡é‡å¯ llm-link è¿›ç¨‹
pub async fn update_config_for_restart(
    State(_state): State<AppState>,
    Json(request): Json<UpdateConfigRequest>,
) -> Result<Json<serde_json::Value>, StatusCode> {
    info!("ğŸ”§ Preparing config update for provider: {}", request.provider);
    
    // éªŒè¯ provider å’Œç”Ÿæˆé»˜è®¤ model
    let default_model = request.model.clone().or_else(|| {
        match request.provider.as_str() {
            "openai" => Some("gpt-4o".to_string()),
            "anthropic" => Some("claude-3-5-sonnet-20241022".to_string()),
            "zhipu" => Some("glm-4-flash".to_string()),
            "ollama" => Some("llama2".to_string()),
            "aliyun" => Some("qwen-turbo".to_string()),
            "volcengine" => Some("ep-20241023xxxxx-xxxxx".to_string()),
            "tencent" => Some("hunyuan-lite".to_string()),
            _ => None,
        }
    });
    
    let model = match default_model {
        Some(m) => m,
        None => {
            error!("âŒ Unknown provider: {}", request.provider);
            return Err(StatusCode::BAD_REQUEST);
        }
    };
    
    // æ„å»ºç¯å¢ƒå˜é‡
    let mut env_vars = serde_json::Map::new();
    
    // æ·»åŠ  provider å¯¹åº”çš„ API key ç¯å¢ƒå˜é‡
    let api_key_var = match request.provider.as_str() {
        "openai" => "OPENAI_API_KEY",
        "anthropic" => "ANTHROPIC_API_KEY",
        "zhipu" => "ZHIPU_API_KEY",
        "aliyun" => "ALIYUN_API_KEY",
        "volcengine" => "VOLCENGINE_API_KEY",
        "tencent" => "TENCENT_API_KEY",
        "ollama" => "", // Ollama ä¸éœ€è¦ API key
        _ => return Err(StatusCode::BAD_REQUEST),
    };
    
    if !api_key_var.is_empty() {
        env_vars.insert(api_key_var.to_string(), json!(request.api_key));
    }
    
    // æ·»åŠ  base_urlï¼ˆå¦‚æœæä¾›ï¼‰
    if let Some(base_url) = request.base_url {
        let base_url_var = match request.provider.as_str() {
            "openai" => "OPENAI_BASE_URL",
            "zhipu" => "ZHIPU_BASE_URL",
            "ollama" => "OLLAMA_BASE_URL",
            _ => "",
        };
        if !base_url_var.is_empty() {
            env_vars.insert(base_url_var.to_string(), json!(base_url));
        }
    }
    
    info!("âœ… Config prepared for restart with provider: {}", request.provider);
    
    Ok(Json(json!({
        "status": "success",
        "message": format!("Config prepared for provider: {}", request.provider),
        "restart_required": true,
        "current_instance_id": get_instance_id(),
        "env_vars": env_vars,
        "cli_args": {
            "provider": request.provider,
            "model": model,
        }
    })))
}

/// éªŒè¯ API Key æ˜¯å¦æœ‰æ•ˆ
/// 
/// é€šè¿‡å°è¯•åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ Service å¹¶åˆ—å‡ºæ¨¡å‹æ¥éªŒè¯
pub async fn validate_key(
    State(_state): State<AppState>,
    Json(request): Json<UpdateConfigRequest>,
) -> Result<Json<serde_json::Value>, StatusCode> {
    use crate::settings::LlmBackendSettings;
    use crate::service::Service;
    
    info!("ğŸ” Validating API key for provider: {} (key: {})", request.provider, mask_api_key(&request.api_key));
    
    // æ„å»ºæµ‹è¯•ç”¨çš„ backend settings
    let model = request.model.clone().unwrap_or_else(|| "test-model".to_string());
    
    let test_backend = match request.provider.as_str() {
        "openai" => LlmBackendSettings::OpenAI {
            api_key: request.api_key.clone(),
            base_url: request.base_url.clone(),
            model,
        },
        "anthropic" => LlmBackendSettings::Anthropic {
            api_key: request.api_key.clone(),
            model,
        },
        "zhipu" => LlmBackendSettings::Zhipu {
            api_key: request.api_key.clone(),
            base_url: request.base_url.clone(),
            model,
        },
        "ollama" => LlmBackendSettings::Ollama {
            base_url: request.base_url.clone(),
            model,
        },
        "aliyun" => LlmBackendSettings::Aliyun {
            api_key: request.api_key.clone(),
            model,
        },
        "volcengine" => LlmBackendSettings::Volcengine {
            api_key: request.api_key.clone(),
            model,
        },
        "tencent" => LlmBackendSettings::Tencent {
            api_key: request.api_key.clone(),
            model,
        },
        _ => {
            error!("âŒ Unsupported provider: {}", request.provider);
            return Err(StatusCode::BAD_REQUEST);
        }
    };
    
    // å°è¯•åˆ›å»º service å¹¶åˆ—å‡ºæ¨¡å‹
    match Service::new(&test_backend) {
        Ok(service) => {
            match service.list_models().await {
                Ok(models) => {
                    info!("âœ… API key validated successfully, found {} models", models.len());
                    Ok(Json(json!({
                        "status": "valid",
                        "message": "API key is valid",
                        "models": models.iter().map(|m| &m.id).collect::<Vec<_>>(),
                    })))
                }
                Err(e) => {
                    error!("âŒ API key validation failed: {:?}", e);
                    Ok(Json(json!({
                        "status": "invalid",
                        "message": format!("Failed to list models: {}", e),
                    })))
                }
            }
        }
        Err(e) => {
            error!("âŒ Failed to create service: {:?}", e);
            Ok(Json(json!({
                "status": "error",
                "message": format!("Failed to create service: {}", e),
            })))
        }
    }
}

/// è·å–å½“å‰è¿›ç¨‹ PID
/// 
/// z-agent å¯ä»¥ä½¿ç”¨è¿™ä¸ª PID æ¥ç®¡ç†è¿›ç¨‹ï¼ˆå¦‚é‡å¯ï¼‰
pub async fn get_pid() -> Json<serde_json::Value> {
    let pid = std::process::id();
    
    Json(json!({
        "pid": pid,
        "message": "Use this PID to restart the service"
    }))
}

/// éªŒè¯ API Keyï¼ˆç”¨äºçƒ­æ›´æ–°ï¼‰
///
/// ä¸“é—¨ç”¨äºçƒ­æ›´æ–°åœºæ™¯çš„ API Key éªŒè¯
pub async fn validate_key_for_update(
    State(_state): State<AppState>,
    Json(request): Json<UpdateKeyRequest>,
) -> Result<Json<serde_json::Value>, StatusCode> {
    use crate::settings::LlmBackendSettings;
    use crate::service::Service;

    info!("ğŸ” Validating API key for hot update - provider: {} (key: {})", request.provider, mask_api_key(&request.api_key));

    // ä½¿ç”¨é»˜è®¤æ¨¡å‹è¿›è¡Œæµ‹è¯•
    let model = match request.provider.as_str() {
        "openai" => "gpt-4o".to_string(),
        "anthropic" => "claude-3-5-sonnet-20241022".to_string(),
        "zhipu" => "glm-4-flash".to_string(),
        "ollama" => "llama2".to_string(),
        "aliyun" => "qwen-turbo".to_string(),
        "volcengine" => "ep-20241023xxxxx-xxxxx".to_string(),
        "tencent" => "hunyuan-lite".to_string(),
        _ => {
            error!("âŒ Unsupported provider: {}", request.provider);
            return Err(StatusCode::BAD_REQUEST);
        }
    };

    let test_backend = match request.provider.as_str() {
        "openai" => LlmBackendSettings::OpenAI {
            api_key: request.api_key.clone(),
            base_url: request.base_url.clone(),
            model,
        },
        "anthropic" => LlmBackendSettings::Anthropic {
            api_key: request.api_key.clone(),
            model,
        },
        "zhipu" => LlmBackendSettings::Zhipu {
            api_key: request.api_key.clone(),
            base_url: request.base_url.clone(),
            model,
        },
        "ollama" => LlmBackendSettings::Ollama {
            base_url: request.base_url.clone(),
            model,
        },
        "aliyun" => LlmBackendSettings::Aliyun {
            api_key: request.api_key.clone(),
            model,
        },
        "volcengine" => LlmBackendSettings::Volcengine {
            api_key: request.api_key.clone(),
            model,
        },
        "tencent" => LlmBackendSettings::Tencent {
            api_key: request.api_key.clone(),
            model,
        },
        _ => {
            error!("âŒ Unsupported provider: {}", request.provider);
            return Err(StatusCode::BAD_REQUEST);
        }
    };

    // å°è¯•åˆ›å»º service å¹¶åˆ—å‡ºæ¨¡å‹
    match Service::new(&test_backend) {
        Ok(service) => {
            match service.list_models().await {
                Ok(models) => {
                    info!("âœ… API key validated successfully for hot update, found {} models", models.len());
                    Ok(Json(json!({
                        "status": "valid",
                        "message": "API key is valid and ready for hot update",
                        "provider": request.provider,
                        "models": models.iter().map(|m| &m.id).collect::<Vec<_>>(),
                        "supports_hot_reload": true,
                    })))
                }
                Err(e) => {
                    error!("âŒ API key validation failed for hot update: {:?}", e);
                    Ok(Json(json!({
                        "status": "invalid",
                        "message": format!("Failed to list models: {}", e),
                        "provider": request.provider,
                    })))
                }
            }
        }
        Err(e) => {
            error!("âŒ Failed to create service for hot update validation: {:?}", e);
            Ok(Json(json!({
                "status": "error",
                "message": format!("Failed to create service: {}", e),
                "provider": request.provider,
            })))
        }
    }
}

/// è¿è¡Œæ—¶æ›´æ–° API Key
///
/// è¿™ä¸ªç«¯ç‚¹å…è®¸åœ¨ä¸é‡å¯æœåŠ¡çš„æƒ…å†µä¸‹æ›´æ–°æŒ‡å®š provider çš„ API Key
pub async fn update_key(
    State(state): State<AppState>,
    Json(request): Json<UpdateKeyRequest>,
) -> Result<Json<serde_json::Value>, StatusCode> {
    // éªŒè¯è¾“å…¥
    if let Err(e) = validate_provider(&request.provider) {
        error!("âŒ Invalid provider: {}", e);
        return Err(StatusCode::BAD_REQUEST);
    }

    if request.provider != "ollama" {
        if let Err(e) = validate_api_key(&request.provider, &request.api_key) {
            error!("âŒ Invalid API key format: {}", e);
            return Ok(Json(json!({
                "status": "error",
                "message": format!("Invalid API key format: {}", e),
            })));
        }
    }

    info!("ğŸ”§ Updating API key for provider: {} (key: {})", request.provider, mask_api_key(&request.api_key));

    // è·å–å½“å‰é…ç½®
    let current_config = state.get_current_config();

    // æ„å»ºæ–°çš„ backend settings
    let new_backend = match request.provider.as_str() {
        "openai" => {
            if let crate::settings::LlmBackendSettings::OpenAI { model, .. } = &current_config.llm_backend {
                crate::settings::LlmBackendSettings::OpenAI {
                    api_key: request.api_key.clone(),
                    base_url: request.base_url.clone(),
                    model: model.clone(),
                }
            } else {
                // å¦‚æœå½“å‰ä¸æ˜¯ OpenAIï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
                crate::settings::LlmBackendSettings::OpenAI {
                    api_key: request.api_key.clone(),
                    base_url: request.base_url.clone(),
                    model: "gpt-4o".to_string(),
                }
            }
        }
        "anthropic" => {
            if let crate::settings::LlmBackendSettings::Anthropic { model, .. } = &current_config.llm_backend {
                crate::settings::LlmBackendSettings::Anthropic {
                    api_key: request.api_key.clone(),
                    model: model.clone(),
                }
            } else {
                crate::settings::LlmBackendSettings::Anthropic {
                    api_key: request.api_key.clone(),
                    model: "claude-3-5-sonnet-20241022".to_string(),
                }
            }
        }
        "zhipu" => {
            if let crate::settings::LlmBackendSettings::Zhipu { model, .. } = &current_config.llm_backend {
                crate::settings::LlmBackendSettings::Zhipu {
                    api_key: request.api_key.clone(),
                    base_url: request.base_url.clone(),
                    model: model.clone(),
                }
            } else {
                crate::settings::LlmBackendSettings::Zhipu {
                    api_key: request.api_key.clone(),
                    base_url: request.base_url.clone(),
                    model: "glm-4-flash".to_string(),
                }
            }
        }
        "aliyun" => {
            if let crate::settings::LlmBackendSettings::Aliyun { model, .. } = &current_config.llm_backend {
                crate::settings::LlmBackendSettings::Aliyun {
                    api_key: request.api_key.clone(),
                    model: model.clone(),
                }
            } else {
                crate::settings::LlmBackendSettings::Aliyun {
                    api_key: request.api_key.clone(),
                    model: "qwen-turbo".to_string(),
                }
            }
        }
        "volcengine" => {
            if let crate::settings::LlmBackendSettings::Volcengine { model, .. } = &current_config.llm_backend {
                crate::settings::LlmBackendSettings::Volcengine {
                    api_key: request.api_key.clone(),
                    model: model.clone(),
                }
            } else {
                crate::settings::LlmBackendSettings::Volcengine {
                    api_key: request.api_key.clone(),
                    model: "ep-20241023xxxxx-xxxxx".to_string(),
                }
            }
        }
        "tencent" => {
            if let crate::settings::LlmBackendSettings::Tencent { model, .. } = &current_config.llm_backend {
                crate::settings::LlmBackendSettings::Tencent {
                    api_key: request.api_key.clone(),
                    model: model.clone(),
                }
            } else {
                crate::settings::LlmBackendSettings::Tencent {
                    api_key: request.api_key.clone(),
                    model: "hunyuan-lite".to_string(),
                }
            }
        }
        "ollama" => {
            if let crate::settings::LlmBackendSettings::Ollama { model, .. } = &current_config.llm_backend {
                crate::settings::LlmBackendSettings::Ollama {
                    base_url: request.base_url.clone(),
                    model: model.clone(),
                }
            } else {
                crate::settings::LlmBackendSettings::Ollama {
                    base_url: request.base_url.clone(),
                    model: "llama2".to_string(),
                }
            }
        }
        _ => {
            error!("âŒ Unsupported provider: {}", request.provider);
            return Err(StatusCode::BAD_REQUEST);
        }
    };

    // å°è¯•æ›´æ–°æœåŠ¡
    match state.update_llm_service(&new_backend) {
        Ok(()) => {
            info!("âœ… API key updated successfully for provider: {}", request.provider);
            Ok(Json(json!({
                "status": "success",
                "message": format!("API key updated for provider: {}", request.provider),
                "provider": request.provider,
                "restart_required": false,
            })))
        }
        Err(e) => {
            error!("âŒ Failed to update API key: {:?}", e);
            Ok(Json(json!({
                "status": "error",
                "message": format!("Failed to update API key: {}", e),
            })))
        }
    }
}

/// åˆ‡æ¢ Provider
///
/// è¿™ä¸ªç«¯ç‚¹å…è®¸åŠ¨æ€åˆ‡æ¢å½“å‰ä½¿ç”¨çš„ LLM æœåŠ¡å•†
pub async fn switch_provider(
    State(state): State<AppState>,
    Json(request): Json<SwitchProviderRequest>,
) -> Result<Json<serde_json::Value>, StatusCode> {
    // éªŒè¯è¾“å…¥
    if let Err(e) = validate_provider(&request.provider) {
        error!("âŒ Invalid provider: {}", e);
        return Err(StatusCode::BAD_REQUEST);
    }

    let masked_key = request.api_key.as_ref().map(|k| mask_api_key(k)).unwrap_or_else(|| "none".to_string());
    info!("ğŸ”„ Switching to provider: {} (key: {})", request.provider, masked_key);

    // è·å–å½“å‰é…ç½®
    let current_config = state.get_current_config();

    // ç¡®å®š API key
    let api_key = if let Some(key) = request.api_key {
        key
    } else {
        // å°è¯•ä»å½“å‰é…ç½®ä¸­è·å–å¯¹åº” provider çš„ API key
        match request.provider.as_str() {
            "openai" => {
                if let crate::settings::LlmBackendSettings::OpenAI { api_key, .. } = &current_config.llm_backend {
                    api_key.clone()
                } else {
                    error!("âŒ No API key provided for OpenAI and none found in current config");
                    return Err(StatusCode::BAD_REQUEST);
                }
            }
            "anthropic" => {
                if let crate::settings::LlmBackendSettings::Anthropic { api_key, .. } = &current_config.llm_backend {
                    api_key.clone()
                } else {
                    error!("âŒ No API key provided for Anthropic and none found in current config");
                    return Err(StatusCode::BAD_REQUEST);
                }
            }
            "zhipu" => {
                if let crate::settings::LlmBackendSettings::Zhipu { api_key, .. } = &current_config.llm_backend {
                    api_key.clone()
                } else {
                    error!("âŒ No API key provided for Zhipu and none found in current config");
                    return Err(StatusCode::BAD_REQUEST);
                }
            }
            "ollama" => String::new(), // Ollama ä¸éœ€è¦ API key
            _ => {
                error!("âŒ Unsupported provider: {}", request.provider);
                return Err(StatusCode::BAD_REQUEST);
            }
        }
    };

    // ç¡®å®šæ¨¡å‹
    let model = request.model.unwrap_or_else(|| {
        match request.provider.as_str() {
            "openai" => "gpt-4o".to_string(),
            "anthropic" => "claude-3-5-sonnet-20241022".to_string(),
            "zhipu" => "glm-4-flash".to_string(),
            "ollama" => "llama2".to_string(),
            "aliyun" => "qwen-turbo".to_string(),
            "volcengine" => "ep-20241023xxxxx-xxxxx".to_string(),
            "tencent" => "hunyuan-lite".to_string(),
            _ => "default-model".to_string(),
        }
    });

    // æ„å»ºæ–°çš„ backend settings
    let new_backend = match request.provider.as_str() {
        "openai" => crate::settings::LlmBackendSettings::OpenAI {
            api_key,
            base_url: request.base_url,
            model,
        },
        "anthropic" => crate::settings::LlmBackendSettings::Anthropic {
            api_key,
            model,
        },
        "zhipu" => crate::settings::LlmBackendSettings::Zhipu {
            api_key,
            base_url: request.base_url,
            model,
        },
        "ollama" => crate::settings::LlmBackendSettings::Ollama {
            base_url: request.base_url,
            model,
        },
        "aliyun" => crate::settings::LlmBackendSettings::Aliyun {
            api_key,
            model,
        },
        "volcengine" => crate::settings::LlmBackendSettings::Volcengine {
            api_key,
            model,
        },
        "tencent" => crate::settings::LlmBackendSettings::Tencent {
            api_key,
            model,
        },
        _ => {
            error!("âŒ Unsupported provider: {}", request.provider);
            return Err(StatusCode::BAD_REQUEST);
        }
    };

    // å°è¯•æ›´æ–°æœåŠ¡
    match state.update_llm_service(&new_backend) {
        Ok(()) => {
            info!("âœ… Provider switched successfully to: {}", request.provider);
            Ok(Json(json!({
                "status": "success",
                "message": format!("Provider switched to: {}", request.provider),
                "provider": request.provider,
                "model": new_backend.get_model(),
                "restart_required": false,
            })))
        }
        Err(e) => {
            error!("âŒ Failed to switch provider: {:?}", e);
            Ok(Json(json!({
                "status": "error",
                "message": format!("Failed to switch provider: {}", e),
            })))
        }
    }
}

/// è§¦å‘ä¼˜é›…å…³é—­
///
/// æ³¨æ„ï¼šè¿™éœ€è¦é…åˆä¿¡å·å¤„ç†æ‰èƒ½å®ç°ä¼˜é›…å…³é—­
/// z-agent åº”è¯¥å…ˆè°ƒç”¨æ­¤ç«¯ç‚¹ï¼Œç­‰å¾…å“åº”åå†å¯åŠ¨æ–°è¿›ç¨‹
pub async fn shutdown() -> Json<serde_json::Value> {
    info!("ğŸ›‘ Shutdown requested via API");

    // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥è§¦å‘ä¼˜é›…å…³é—­
    // ç›®å‰åªè¿”å›æˆåŠŸï¼Œè®©è°ƒç”¨æ–¹å†³å®šå¦‚ä½•é‡å¯

    Json(json!({
        "status": "success",
        "message": "Shutdown signal sent. Please restart with new configuration.",
    }))
}
