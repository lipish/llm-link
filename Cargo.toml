[package]
name = "llm-link"
version = "0.5.1"
edition = "2021"
authors = ["LLM Link Contributors"]
description = "A universal LLM proxy supporting 10 providers (OpenAI, Anthropic, Zhipu, Aliyun, Volcengine, Tencent, Longcat, Moonshot, Minimax, Ollama) with dynamic model discovery API, hot-reload configuration, and optional API key startup"
license = "MIT"
repository = "https://github.com/lipish/llm-link"
homepage = "https://github.com/lipish/llm-link"
documentation = "https://github.com/lipish/llm-link/blob/master/README.md"
readme = "README.md"
keywords = ["llm", "proxy", "ollama", "openai", "anthropic"]
categories = ["command-line-utilities", "web-programming", "api-bindings"]
exclude = [
    "tests/*",
    "keys.yaml",
    ".gitignore",
    ".git/*",
]

[dependencies]
# Web framework
axum = "0.7"
tokio = { version = "1.0", features = ["full"] }
tower = "0.4"
tower-http = { version = "0.5", features = ["cors", "trace"] }

# LLM connector
llm-connector = { version = "0.5.6", features = ["streaming"] }

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
serde_yaml = "0.9"

# Regex for environment variable expansion
regex = "1.0"

# Error handling
anyhow = "1.0"
thiserror = "1.0"

# Logging
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }

# HTTP client (already included in llm-connector but explicit for clarity)
reqwest = { version = "0.11", features = ["json", "stream"] }

# Utilities
clap = { version = "4.0", features = ["derive"] }
uuid = { version = "1.0", features = ["v4"] }
chrono = { version = "0.4", features = ["serde"] }
futures = "0.3"
tokio-stream = "0.1"
futures-util = "0.3"
once_cell = "1.19"

[dev-dependencies]
tempfile = "3.0"
