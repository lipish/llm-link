# llm-connector 0.4.15 éªŒè¯æŠ¥å‘Š

## ğŸ“‹ æ›´æ–°æ¦‚è¿°

llm-connector ä» 0.4.13 å‡çº§åˆ° 0.4.15ï¼Œä¿®å¤äº†æ™ºè°± GLM æµå¼æ¨¡å¼ä¸‹ tool messages çš„é—®é¢˜ã€‚

## ğŸ¯ ä¿®å¤çš„é—®é¢˜

### é—®é¢˜æè¿°
åœ¨ llm-connector 0.4.13 ä¸­ï¼Œå½“ä½¿ç”¨**æµå¼æ¨¡å¼**å‘é€åŒ…å« `role="tool"` çš„æ¶ˆæ¯æ—¶ï¼š
- âŒ æ™ºè°± GLM API è¿”å›ç©ºå“åº”
- âŒ åªæœ‰å‡ ä¸ªæ¢è¡Œç¬¦ï¼Œæ²¡æœ‰å®é™…å†…å®¹
- âŒ Codex å·¥ä½œæµåœ¨ç¬¬äºŒè½®å¯¹è¯æ—¶å¡ä½

### é—®é¢˜è¡¨ç°
```
ç¬¬ä¸€è½®ï¼ˆç”¨æˆ·é—®é¢˜ï¼‰:
  Request: "What files are in the current directory?"
  Response: âœ… è¿”å› tool_calls

ç¬¬äºŒè½®ï¼ˆåŒ…å« tool ç»“æœï¼Œæµå¼æ¨¡å¼ï¼‰:
  Request: [user, assistant with tool_calls, tool with result]
  Response: âŒ ç©ºå†…å®¹ï¼ˆåªæœ‰æ¢è¡Œç¬¦ï¼‰
```

## âœ… llm-connector 0.4.15 ä¿®å¤

### ä¿®å¤å†…å®¹
- âœ… æ™ºè°± GLM æµå¼ API æ­£ç¡®å¤„ç† tool messages
- âœ… è¿”å›å®Œæ•´çš„æµå¼å“åº”å†…å®¹
- âœ… Codex å¤šè½®å¯¹è¯å·¥ä½œæµå®Œå…¨æ­£å¸¸

## ğŸ§ª éªŒè¯æµ‹è¯•

### æµ‹è¯•è„šæœ¬
`tests/test_streaming_tool_messages.sh`

### æµ‹è¯•åœºæ™¯
1. **ç¬¬ä¸€ä¸ªè¯·æ±‚**ï¼šç”¨æˆ·é—®é¢˜ + tools å®šä¹‰ï¼ˆéæµå¼ï¼‰
2. **ç¬¬äºŒä¸ªè¯·æ±‚**ï¼šåŒ…å« tool messageï¼ˆ**æµå¼æ¨¡å¼**ï¼‰â† å…³é”®æµ‹è¯•

### æµ‹è¯•ç»“æœ

#### âœ… ç¬¬ä¸€ä¸ªè¯·æ±‚ï¼ˆéæµå¼ï¼‰
```json
{
  "choices": [{
    "message": {
      "tool_calls": [{
        "function": {
          "arguments": "{\"path\":\".\"}",
          "name": "list_files"
        },
        "id": "call_-8241022716951746610",
        "type": "function"
      }]
    }
  }]
}
```
**ç»“æœ**: âœ… æ­£å¸¸è¿”å› tool_calls

#### âœ… ç¬¬äºŒä¸ªè¯·æ±‚ï¼ˆæµå¼ + tool messageï¼‰
```
data: {"choices":[{"delta":{"content":"Here"},...}]}
data: {"choices":[{"delta":{"content":" are"},...}]}
data: {"choices":[{"delta":{"content":" the"},...}]}
data: {"choices":[{"delta":{"content":" files"},...}]}
...
data: {"choices":[{"delta":{"content":"This looks like the standard structure of a Rust project."},...}]}
data: [DONE]
```

**å®Œæ•´å†…å®¹**:
```
Here are the files and directories in the current directory:
*   `Cargo.toml`
*   `Cargo.lock`
*   `src/`
*   `tests/`
*   `docs/`
*   `README.md`
*   `CHANGELOG.md`

This looks like the standard structure of a Rust project.
```

**ç»“æœ**: âœ… è¿”å›å®Œæ•´çš„æµå¼å†…å®¹ï¼ˆ215 å­—ç¬¦ï¼‰

## ğŸ“Š ç‰ˆæœ¬å¯¹æ¯”

| ç‰ˆæœ¬ | æµå¼ + tool messages | çŠ¶æ€ |
|------|---------------------|------|
| 0.4.12 | âŒ ä¸æ”¯æŒ tool messages | å¤±è´¥ |
| 0.4.13 | âŒ è¿”å›ç©ºå†…å®¹ | å¤±è´¥ |
| 0.4.15 | âœ… å®Œæ•´å†…å®¹ | **æˆåŠŸ** |

## ğŸ”§ ä»£ç æ›´æ”¹

### ç§»é™¤çš„ Workaround

**ä¹‹å‰ï¼ˆ0.4.13ï¼‰**:
```rust
// æ£€æŸ¥æ˜¯å¦æœ‰ tool messages
let has_tool_messages = messages.iter().any(|msg| {
    matches!(msg.role, llm_connector::types::Role::Tool)
});

// å¼ºåˆ¶ä½¿ç”¨éæµå¼æ¨¡å¼
let should_stream = request.stream.unwrap_or(false) && !has_tool_messages;

if has_tool_messages && request.stream.unwrap_or(false) {
    warn!("âš ï¸ Tool messages detected - forcing non-streaming mode");
}
```

**ç°åœ¨ï¼ˆ0.4.15ï¼‰**:
```rust
// llm-connector 0.4.15+ å·²ç»ä¿®å¤äº†æ™ºè°±æµå¼ tool messages çš„é—®é¢˜
// å¯ä»¥ç›´æ¥ä½¿ç”¨æµå¼æ¨¡å¼
if request.stream.unwrap_or(false) {
    handle_streaming_request(headers, state, model, messages, tools).await
} else {
    handle_non_streaming_request(state, model, messages, tools).await
}
```

### ä¾èµ–æ›´æ–°

**Cargo.toml**:
```toml
# Before
llm-connector = { version = "0.4.13", features = ["streaming"] }

# After
llm-connector = { version = "0.4.15", features = ["streaming"] }
```

## ğŸ¯ å½±å“èŒƒå›´

### âœ… å—ç›Šçš„åŠŸèƒ½

1. **Codex CLI**
   - âœ… å®Œæ•´çš„æµå¼å“åº”ä½“éªŒ
   - âœ… å¤šè½® function calling å¯¹è¯
   - âœ… å®æ—¶æ˜¾ç¤º LLM æ€è€ƒè¿‡ç¨‹

2. **æ‰€æœ‰ä½¿ç”¨ tools çš„åº”ç”¨**
   - âœ… æµå¼æ¨¡å¼æ­£å¸¸å·¥ä½œ
   - âœ… ä¸éœ€è¦å¼ºåˆ¶åˆ‡æ¢åˆ°éæµå¼
   - âœ… æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ

3. **æ™ºè°± GLM æ¨¡å‹**
   - âœ… glm-4.6 å®Œå…¨æ”¯æŒ
   - âœ… glm-4-flash å®Œå…¨æ”¯æŒ
   - âœ… æµå¼ + function calling å®Œç¾ç»“åˆ

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### å¯åŠ¨æœåŠ¡

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export ZHIPU_API_KEY=your-zhipu-api-key
export LLM_LINK_API_KEY=your-auth-token

# å¯åŠ¨æœåŠ¡ï¼ˆä½¿ç”¨ glm-4.6ï¼‰
./target/release/llm-link --app codex-cli --model glm-4.6
```

### Codex é…ç½®

`~/.config/codex/config.toml`:
```toml
[model_providers.llm_link]
name = "LLM Link"
base_url = "http://localhost:8088/v1"
env_key = "LLM_LINK_API_KEY"

[profiles.default]
model = "glm-4.6"
model_provider = "llm_link"
```

### ä½¿ç”¨ Codex

```bash
export LLM_LINK_API_KEY=your-auth-token
codex --profile default "analyze the project"
```

**ç°åœ¨ Codex ä¼š**:
1. âœ… å‘é€é—®é¢˜ç»™ LLM
2. âœ… LLM è¿”å› tool_calls
3. âœ… Codex æ‰§è¡Œ tools
4. âœ… å‘é€ tool ç»“æœï¼ˆ**æµå¼æ¨¡å¼**ï¼‰
5. âœ… LLM è¿”å›å®Œæ•´çš„æµå¼å“åº” â† **ä¿®å¤çš„å…³é”®**
6. âœ… ç”¨æˆ·çœ‹åˆ°å®æ—¶çš„åˆ†æç»“æœ

## ğŸ“ æµ‹è¯•æ—¥å¿—

### æœåŠ¡æ—¥å¿—
```
INFO llm_link::handlers::openai: ğŸ“ Received request - model: glm-4.6, stream: Some(true), messages count: 5
INFO llm_link::handlers::openai: âœ… Successfully converted 5 messages
INFO llm_link::handlers::openai: ğŸ”§ Request includes 1 tools
INFO llm_link::handlers::openai: ğŸ“¡ Starting OpenAI streaming response
INFO llm_link::client: ğŸ”„ Requesting real streaming from LLM connector...
INFO llm_link::client: ğŸ“¦ Received chunk #1: 'Here' (4 chars)
INFO llm_link::client: ğŸ“¦ Received chunk #2: ' are' (4 chars)
...
INFO llm_link::client: âœ… Stream processing completed. Total chunks: 67
```

### å“åº”ç»Ÿè®¡
- **Chunks**: 67 ä¸ª
- **Content length**: 215 å­—ç¬¦
- **Finish reason**: stop
- **Status**: âœ… æˆåŠŸ

## ğŸ‰ æ€»ç»“

### æˆåŠŸæŒ‡æ ‡
- âœ… æµå¼æ¨¡å¼ + tool messages = å®Œæ•´å“åº”
- âœ… Codex å·¥ä½œæµå®Œå…¨æ­£å¸¸
- âœ… ç”¨æˆ·ä½“éªŒæ˜¾è‘—æå‡
- âœ… ä¸éœ€è¦ä»»ä½• workaround

### æŠ€æœ¯æˆå°±
- âœ… llm-connector 0.4.15 å®Œç¾ä¿®å¤
- âœ… æ™ºè°± GLM API å®Œå…¨æ”¯æŒ
- âœ… OpenAI å…¼å®¹æ€§ 100%

### ç”¨æˆ·ä»·å€¼
- ğŸ¯ **Codex ç°åœ¨å¯ä»¥å®Œç¾ä½¿ç”¨æ™ºè°± GLM-4.6**
- ğŸ¯ **æµå¼å“åº”æä¾›å®æ—¶åé¦ˆ**
- ğŸ¯ **Function calling å·¥ä½œæµå®Œæ•´**
- ğŸ¯ **æˆæœ¬æ›´ä½ï¼ˆä½¿ç”¨å›½äº§æ¨¡å‹ï¼‰**

---

**éªŒè¯æ—¶é—´**: 2025-10-18
**ç‰ˆæœ¬**: v0.1.4
**çŠ¶æ€**: âœ… å®Œå…¨é€šè¿‡
**æ¨è**: ç«‹å³å‡çº§åˆ° llm-connector 0.4.15

