# Release v0.2.0 - Multi-Modal Support & Claude Code Integration

**Release Date**: 2025-10-21  
**Version**: 0.2.0  
**Status**: âœ… Published

## ğŸ‰ Release Summary

This is a **major feature release** bringing multi-modal support (text + images) and Claude Code integration to llm-link!

### Key Highlights

- ğŸ–¼ï¸ **Multi-Modal Support** - Text + Images via llm-connector 0.5.1
- ğŸ¤– **Claude Code Integration** - Smart streaming detection via Accept header
- ğŸ§ª **Comprehensive Testing** - Zhipu, Aliyun, Volcengine providers tested
- âœ… **Fully Backward Compatible** - No breaking changes!

## ğŸ“¦ Installation

### From crates.io
```bash
cargo install llm-link
```

### From GitHub
```bash
git clone https://github.com/lipish/llm-link.git
cd llm-link
git checkout v0.2.0
cargo build --release
```

### From Binary (Coming Soon)
Pre-built binaries will be available on the [Releases](https://github.com/lipish/llm-link/releases/tag/v0.2.0) page.

## ğŸ¯ What's New

### 1. Multi-Modal Support ğŸ–¼ï¸

**Upgraded to llm-connector 0.5.1** with native multi-modal content support!

#### Before (v0.1.x)
```rust
Message {
    content: "What's in this image?"  // âŒ Images discarded
}
```

#### After (v0.2.0)
```rust
Message {
    content: [
        MessageBlock::Text { text: "What's in this image?" },
        MessageBlock::Image {
            source: ImageSource::Base64 {
                media_type: "image/jpeg",
                data: "..."
            }
        }
    ]
}
```

#### Features
- âœ… **Text + Images** - Send images alongside text
- âœ… **Base64 Support** - Inline image data
- âœ… **Anthropic API** - Full multi-modal support
- âœ… **Future-Proof** - Ready for documents, audio, video

#### Example Usage
```bash
curl -X POST http://localhost:8089/v1/messages \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-3.5-sonnet",
    "messages": [{
      "role": "user",
      "content": [
        {"type": "text", "text": "What is in this image?"},
        {"type": "image", "source": {"type": "base64", "media_type": "image/jpeg", "data": "..."}}
      ]
    }]
  }'
```

### 2. Claude Code Integration ğŸ¤–

**Smart streaming detection** via HTTP Accept header!

#### The Problem
Claude Code sends:
- Request body: `"stream": false`
- HTTP header: `Accept: text/event-stream`

It expects streaming responses but doesn't set `stream: true`!

#### The Solution
Detect streaming preference via Accept header:

```rust
// Check Accept header
if headers.get("accept").contains("text/event-stream") {
    // Enable streaming even if stream=false
    request.stream = true;
}
```

#### Benefits
- âœ… **Works with Claude Code** - Proper streaming responses
- âœ… **HTTP Standard** - Uses content negotiation
- âœ… **No Breaking Changes** - Non-streaming still works
- âœ… **Smart Detection** - Respects client preferences

### 3. Comprehensive Provider Testing ğŸ§ª

Tested 3 major Chinese LLM providers:

| Provider | Non-Streaming | Streaming | Status |
|----------|---------------|-----------|--------|
| **Zhipu GLM-4-Flash** | âœ… Perfect | âœ… Perfect | âœ… **Recommended** |
| **Aliyun Qwen-Max** | âœ… Perfect | âœ… Perfect | âœ… **Recommended** |
| **Volcengine Doubao** | âœ… Works | âš ï¸ Empty | âš ï¸ **Use Non-Streaming** |

See [PROVIDER_TEST_REPORT.md](PROVIDER_TEST_REPORT.md) for details.

### 4. Code Quality Improvements âœ¨

#### New Helper Methods
```rust
// Extract text from multi-modal content
let text = message.content_as_text();

// Check content type
if message.is_text_only() { ... }
if message.has_images() { ... }
```

#### Better Logging
```
ğŸ“¨ Anthropic Messages API request: client_model=glm-4.6, stream=false
ğŸ“‹ Accept: Some("text/event-stream")
ğŸ”„ Client expects SSE, enabling streaming
ğŸ“‹ Final streaming mode: true
âœ… Starting Anthropic streaming response
```

#### Removed Warnings
- Fixed dead code warnings in Anthropic API
- Cleaner compilation output

## ğŸ”§ Bug Fixes

### 1. Model Name Passing
**Before**: Client-requested model names were ignored  
**After**: Model names properly passed to backend

```rust
// Now correctly passes client's model choice
state.llm_service.chat(Some(&request.model), messages, None)
```

### 2. Streaming Detection
**Before**: Forced all requests to stream  
**After**: Smart detection via Accept header

### 3. Multi-Modal Content
**Before**: Images discarded, only text preserved  
**After**: All content blocks preserved

## ğŸ“Š Technical Details

### Dependencies Updated
- **llm-connector**: 0.4.20 â†’ 0.5.1

### API Changes (Internal)
```rust
// Message structure changed
pub struct Message {
    pub content: Vec<MessageBlock>,  // Was: String
    // ... other fields
}

// New types
pub enum MessageBlock {
    Text { text: String },
    Image { source: ImageSource },
}

pub enum ImageSource {
    Base64 { media_type: String, data: String },
    Url { url: String },
}
```

### Breaking Changes
**None!** This release is fully backward compatible.

- Existing code works without changes
- All CLI options remain the same
- Configuration files compatible

## ğŸ§ª Testing

### Automated Tests
```bash
cargo test
```

### Manual Testing
```bash
# Start service
./target/release/llm-link --app claude-code --provider zhipu --model glm-4-flash

# Test streaming
curl -N -X POST http://localhost:8089/v1/messages \
  -H "Accept: text/event-stream" \
  -d '{"model":"glm-4-flash","messages":[{"role":"user","content":"Hi"}]}'

# Test non-streaming
curl -X POST http://localhost:8089/v1/messages \
  -H "Accept: application/json" \
  -d '{"model":"glm-4-flash","messages":[{"role":"user","content":"Hi"}]}'
```

### Provider Testing
See [PROVIDER_TEST_REPORT.md](PROVIDER_TEST_REPORT.md) for comprehensive test results.

## ğŸ“š Documentation

### New Documents
- **UPGRADE_v0.5.1.md** - Comprehensive upgrade guide
- **PROVIDER_TEST_REPORT.md** - Detailed provider test results
- **RELEASE_v0.2.0.md** - This release notes

### Updated Documents
- **CHANGELOG.md** - Full changelog
- **README.md** - Updated examples

## ğŸš€ Migration Guide

### For Users
**No changes needed!** Just update and run:

```bash
cargo install llm-link --force
```

### For Developers
If you use llm-connector directly:

```rust
// Before (0.1.x)
let content: String = msg.content;

// After (0.2.0)
let content: String = msg.content_as_text();
```

## ğŸ¯ Recommendations

### Production Use
**Recommended Providers**:
1. **Zhipu GLM-4-Flash** - Fast, reliable, cost-effective
2. **Aliyun Qwen-Max** - Powerful, excellent Chinese support

**Configuration**:
```bash
# Zhipu (Recommended)
export ZHIPU_API_KEY="your-key"
./llm-link --app zed --provider zhipu --model glm-4-flash

# Aliyun (Alternative)
export ALIYUN_API_KEY="your-key"
./llm-link --app zed --provider aliyun --model qwen-max
```

### Known Issues
- **Volcengine Streaming**: Returns empty content (use non-streaming)
- **Tencent**: API key needs verification
- **OpenAI/Anthropic**: Pending testing (no API keys)

## ğŸ”— Links

- **GitHub**: https://github.com/lipish/llm-link
- **Crates.io**: https://crates.io/crates/llm-link
- **Release Tag**: https://github.com/lipish/llm-link/releases/tag/v0.2.0
- **Issues**: https://github.com/lipish/llm-link/issues

## ğŸ™ Acknowledgments

- **llm-connector** team for the excellent 0.5.1 release
- **Claude Code** for inspiring the Accept header fix
- All contributors and testers

## ğŸ“ˆ Statistics

- **Commits**: 15+ commits since v0.1.4
- **Files Changed**: 10+ files
- **Lines Added**: 500+ lines
- **Lines Removed**: 100+ lines
- **Test Coverage**: 3 providers fully tested

## ğŸ‰ Conclusion

**v0.2.0 is a major milestone** for llm-link!

Key achievements:
- âœ… Multi-modal support enabled
- âœ… Claude Code integration working
- âœ… Comprehensive provider testing
- âœ… Zero breaking changes
- âœ… Better code quality

**Ready for production use** with Zhipu or Aliyun providers!

---

**Published**: 2025-10-21  
**Version**: 0.2.0  
**License**: MIT  
**Author**: LLM Link Contributors

