# Codex CLI Configuration Example for LLM Link
# Save this as ~/.config/codex/config.toml or your Codex config location

# Option 1: Direct connection to Zhipu GLM API
[model_providers.z_ai]
name = "z.ai - GLM Coding Plan"
# Direct connection to Zhipu API (no proxy)
base_url = "https://open.bigmodel.cn/api/paas/v4/"
env_key = "Z_AI_API_KEY"

# Option 2: Connection through LLM Link proxy
[model_providers.llm_link]
name = "LLM Link - GLM Models"
# Connection through LLM Link proxy (with additional features)
base_url = "http://localhost:11434/v1"
env_key = "LLM_LINK_API_KEY"

# Profiles using direct connection (z_ai provider)
[profiles.glm_4_5_direct]
model = "glm-4.5"
model_provider = "z_ai"

# Profiles using LLM Link proxy (llm_link provider)
[profiles.glm_4_flash]
model = "glm-4-flash"
model_provider = "llm_link"

[profiles.glm_4_plus]
model = "glm-4-plus"
model_provider = "llm_link"

[profiles.glm_4]
model = "glm-4"
model_provider = "llm_link"

[profiles.glm_4_air]
model = "glm-4-air"
model_provider = "llm_link"

[profiles.glm_4_long]
model = "glm-4-long"
model_provider = "llm_link"

# Usage examples:
# Direct connection:
# codex --profile glm_4_5_direct "Write a Python function to sort a list"
#
# Through LLM Link proxy:
# codex --profile glm_4_flash "Write a Python function to sort a list"
# codex --profile glm_4_plus "Refactor this code to use async/await"
# codex --profile glm_4_long "Analyze this entire codebase"
