# Zed IDE Integration Guide

## ğŸ¯ Overview

LLM Link provides seamless integration with Zed IDE by exposing an Ollama-compatible API while allowing you to use any supported LLM provider as the backend.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Zed IDE   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Ollama API Format
       â”‚ (http://localhost:11434)
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      LLM Link Proxy             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Ollama API Layer        â”‚  â”‚ â† Receives Ollama format requests
â”‚  â”‚   - /api/chat             â”‚  â”‚
â”‚  â”‚   - /api/tags             â”‚  â”‚
â”‚  â”‚   - /api/generate         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â”‚                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Format Converter        â”‚  â”‚ â† Converts between formats
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â”‚                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   LLM Backend             â”‚  â”‚ â† Connects to actual LLM
â”‚  â”‚   (Aliyun/Zhipu/OpenAI)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Provider API   â”‚
â”‚  - Aliyun Qwen      â”‚
â”‚  - Zhipu GLM        â”‚
â”‚  - OpenAI GPT       â”‚
â”‚  - Anthropic Claude â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Configuration

### Start LLM Link for Zed

```bash
# Using Aliyun Qwen
./llm-link --app zed --provider aliyun --model qwen-max

# Using Zhipu GLM
./llm-link --app zed --provider zhipu --model glm-4-flash

# Using OpenAI
./llm-link --app zed --provider openai --model gpt-4

# Using Anthropic Claude
./llm-link --app zed --provider anthropic --model claude-3-5-sonnet-20241022
```

### Configure Zed IDE

Edit your Zed settings file (`~/.config/zed/settings.json`):

```json
{
  "language_models": {
    "ollama": {
      "api_url": "http://localhost:11434"
    }
  }
}
```

## ğŸ“‹ Understanding the Logs

When you start LLM Link with `--app zed --provider aliyun`, you'll see:

```
ğŸš€ Starting in zed mode
ğŸ”„ Overriding LLM provider to: aliyun
ğŸ”„ Using model: qwen-max
ğŸŒ Server will bind to 0.0.0.0:11434
ğŸ¦™ Ollama API enabled on path: 
ğŸ”“ Ollama API key authentication: DISABLED
```

### Why "Ollama API enabled" when using Aliyun?

This is **correct behavior**! Here's why:

1. **Frontend Protocol**: Zed IDE communicates using Ollama API format
2. **Backend Provider**: LLM Link forwards requests to Aliyun (or any other provider)
3. **Format Translation**: LLM Link automatically converts between Ollama and provider formats

**Think of it as**:
- **Ollama API** = The "language" Zed speaks
- **Aliyun/Zhipu/OpenAI** = The actual AI service doing the work
- **LLM Link** = The translator between them

## ğŸ”„ Request Flow Example

### 1. Zed sends Ollama format request:

```json
POST http://localhost:11434/api/chat
{
  "model": "qwen-max",
  "messages": [
    {"role": "user", "content": "Hello"}
  ],
  "stream": true
}
```

### 2. LLM Link converts to Aliyun format:

```json
POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
{
  "model": "qwen-max",
  "messages": [
    {"role": "user", "content": "Hello"}
  ],
  "stream": true
}
Headers: {
  "Authorization": "Bearer sk-xxx",
  "Content-Type": "application/json"
}
```

### 3. Aliyun responds, LLM Link converts back to Ollama format

### 4. Zed receives Ollama format response

## âœ… Supported Features

- âœ… Chat completions
- âœ… Streaming responses
- âœ… Model listing
- âœ… Multi-turn conversations
- âœ… System prompts
- âœ… Temperature and other parameters

## ğŸ¯ Supported Providers

| Provider | Model Examples | API Key Required |
|----------|---------------|------------------|
| **Aliyun** | qwen-max, qwen-plus, qwen-turbo | Yes (ALIYUN_API_KEY) |
| **Zhipu** | glm-4-flash, glm-4-plus | Yes (ZHIPU_API_KEY) |
| **OpenAI** | gpt-4, gpt-3.5-turbo | Yes (OPENAI_API_KEY) |
| **Anthropic** | claude-3-5-sonnet | Yes (ANTHROPIC_API_KEY) |
| **Ollama** | llama2, mistral, etc. | No (local) |

## ğŸš€ Quick Start

### 1. Set up API key

```bash
# For Aliyun
export ALIYUN_API_KEY="sk-your-key-here"

# For Zhipu
export ZHIPU_API_KEY="your-key-here"

# For OpenAI
export OPENAI_API_KEY="sk-your-key-here"
```

### 2. Start LLM Link

```bash
./llm-link --app zed --provider aliyun --model qwen-max
```

### 3. Configure Zed

Add to `~/.config/zed/settings.json`:

```json
{
  "language_models": {
    "ollama": {
      "api_url": "http://localhost:11434"
    }
  }
}
```

### 4. Use in Zed

- Open Zed IDE
- Use the AI assistant features
- Zed will automatically use your configured LLM provider through LLM Link!

## ğŸ” Troubleshooting

### Issue: "Ollama API enabled" but I'm using Aliyun

**This is normal!** Ollama API is the protocol Zed uses to communicate. Your backend is still Aliyun.

### Issue: Connection refused

Make sure LLM Link is running on port 11434:
```bash
curl http://localhost:11434/api/tags
```

### Issue: No models showing up

Check that your API key is valid and the service is running.

## ğŸ“š See Also

- [Quick Start Guide](QUICK_START.md)
- [Main README](../README.md)
- [Changelog](../CHANGELOG.md)

