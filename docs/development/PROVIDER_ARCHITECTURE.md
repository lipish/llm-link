# LLM-Link ä»£ç ç»“æ„ä¸ Provider è®¾ç½®æœºåˆ¶

## ğŸ“ æ•´ä½“æ¶æ„

```
llm-link
â”œâ”€â”€ main.rs          # ç¨‹åºå…¥å£ï¼Œè·¯ç”±é…ç½®
â”œâ”€â”€ settings.rs      # Provider é…ç½®å®šä¹‰ï¼ˆæšä¸¾ï¼‰
â”œâ”€â”€ cli/             # å‘½ä»¤è¡Œå‚æ•°è§£æå’Œé…ç½®åŠ è½½
â”œâ”€â”€ llm/             # LLM å®¢æˆ·ç«¯å°è£…ï¼ˆè°ƒç”¨ llm-connectorï¼‰
â”œâ”€â”€ service.rs       # æœåŠ¡å±‚ï¼ˆä¸šåŠ¡é€»è¾‘ï¼‰
â”œâ”€â”€ api/             # HTTP API ç«¯ç‚¹
â””â”€â”€ models/          # æ¨¡å‹é…ç½®ï¼ˆYAMLï¼‰
```

## ğŸ”„ Provider è®¾ç½®æµç¨‹

### 1. å‘½ä»¤è¡Œå‚æ•°è§£æ

**å…¥å£**: `main.rs` â†’ `Args::parse()`

```rust
// ç”¨æˆ·å‘½ä»¤ç¤ºä¾‹
./llm-link --app zed --provider minimax
```

**å‚æ•°ç»“æ„** (`cli/args.rs`):
- `--app`: åº”ç”¨æ¨¡å¼ï¼ˆzed, codex-cli ç­‰ï¼‰
- `--provider`: Provider åç§°ï¼ˆminimax, openai ç­‰ï¼‰
- `--model`: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
- `--llm-api-key`: API Keyï¼ˆå¯é€‰ï¼Œä¼˜å…ˆäºç¯å¢ƒå˜é‡ï¼‰

### 2. é…ç½®åŠ è½½æµç¨‹

**æ–‡ä»¶**: `cli/loader.rs`

#### æ­¥éª¤ 1: è§£æåº”ç”¨æ¨¡å¼
```rust
// ç¡®å®šåº”ç”¨é…ç½®ï¼ˆZed, Codex CLI ç­‰ï¼‰
let app = SupportedApp::from_str(app_name)?;
let mut config = AppConfigGenerator::generate_config(&app, api_key);
```

#### æ­¥éª¤ 2: åº”ç”¨ Provider è¦†ç›–
```rust
// æ ¹æ® --provider å‚æ•°è®¾ç½® LLM backend
config = Self::apply_provider_overrides(
    config,
    Some(provider),      // "minimax"
    args.model.as_deref(),  // å¯é€‰æ¨¡å‹åç§°
    args.llm_api_key.as_deref()  // å¯é€‰çš„ API key
)?;
```

#### æ­¥éª¤ 3: Provider è®¾ç½®é€»è¾‘ (`apply_provider_overrides`)

**a) è·å– API Key**
```rust
let api_key = match provider_name {
    "minimax" => std::env::var("MINIMAX_API_KEY").ok(),
    "openai" => std::env::var("OPENAI_API_KEY").ok(),
    // ... å…¶ä»– providers
    "ollama" => None,  // Ollama ä¸éœ€è¦ API key
    _ => return Err("Unknown provider"),
};
```

**b) ç¡®å®šé»˜è®¤æ¨¡å‹**
```rust
let model_name = match provider_name {
    "minimax" => "MiniMax-M2".to_string(),
    "openai" => "gpt-4".to_string(),
    // ... å…¶ä»– providers
};
```

**c) åˆ›å»º Backend Settings**
```rust
config.llm_backend = match provider_name {
    "minimax" => LlmBackendSettings::Minimax {
        api_key: api_key_value,
        model: model_name,
    },
    // ... å…¶ä»– providers
};
```

### 3. Provider ç±»å‹å®šä¹‰

**æ–‡ä»¶**: `settings.rs`

```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type")]
pub enum LlmBackendSettings {
    OpenAI {
        api_key: String,
        base_url: Option<String>,
        model: String,
    },
    Anthropic {
        api_key: String,
        model: String,
    },
    Minimax {
        api_key: String,
        model: String,
    },
    // ... å…¶ä»– 10 ä¸ª providers
}
```

**å…³é”®ç‚¹**:
- ä½¿ç”¨ Rust æšä¸¾ï¼ˆenumï¼‰è¡¨ç¤ºä¸åŒçš„ Provider
- æ¯ä¸ª Provider æœ‰ä¸åŒçš„å­—æ®µç»“æ„
- ä½¿ç”¨ `serde(tag = "type")` æ”¯æŒ JSON åºåˆ—åŒ–

### 4. LLM å®¢æˆ·ç«¯åˆ›å»º

**æ–‡ä»¶**: `llm/mod.rs`

```rust
impl Client {
    pub fn new(config: &LlmBackendSettings) -> Result<Self> {
        let llm_client = match config {
            LlmBackendSettings::Minimax { api_key, .. } => {
                // Minimax ä½¿ç”¨ OpenAI å…¼å®¹ API
                LlmClient::openai_compatible(
                    api_key, 
                    "https://api.minimaxi.com/v1", 
                    "minimax"
                )?
            },
            LlmBackendSettings::OpenAI { api_key, base_url, .. } => {
                if let Some(base_url) = base_url {
                    LlmClient::openai_compatible(api_key, base_url, "openai")?
                } else {
                    LlmClient::openai(api_key)?
                }
            },
            // ... å…¶ä»– providers
        };
        
        Ok(Self {
            backend: config.clone(),
            llm_client,  // å®é™…çš„ HTTP å®¢æˆ·ç«¯
            models_config: ModelsConfig::load_with_fallback(),
        })
    }
}
```

**å…³é”®ç‚¹**:
- æ ¹æ® Provider ç±»å‹åˆ›å»ºä¸åŒçš„ `LlmClient`
- ä½¿ç”¨ `llm-connector` crate å¤„ç†å®é™…çš„ HTTP è¯·æ±‚
- Minimax ä½¿ç”¨ `openai_compatible` æ¨¡å¼ï¼ˆOpenAI å…¼å®¹ APIï¼‰

### 5. æœåŠ¡å±‚åˆå§‹åŒ–

**æ–‡ä»¶**: `service.rs`

```rust
pub struct Service {
    client: Client,      // LLM å®¢æˆ·ç«¯
    model: String,       // é»˜è®¤æ¨¡å‹åç§°
}

impl Service {
    pub fn new(config: &LlmBackendSettings) -> Result<Self> {
        let client = Client::new(config)?;
        let model = match config {
            LlmBackendSettings::Minimax { model, .. } => model.clone(),
            // ... å…¶ä»– providers
        };
        Ok(Self { client, model })
    }
}
```

### 6. åº”ç”¨çŠ¶æ€åˆ›å»º

**æ–‡ä»¶**: `main.rs`

```rust
// åˆå§‹åŒ– LLM æœåŠ¡
let llm_service = initialize_llm_service(&config)?;
let app_state = AppState::new(llm_service, config.clone());
```

**AppState** (`api/mod.rs`):
```rust
pub struct AppState {
    pub llm_service: Arc<RwLock<LlmService>>,  // å¯çº¿ç¨‹å®‰å…¨è®¿é—®
    pub config: Arc<RwLock<Settings>>,          // é…ç½®å¯åŠ¨æ€æ›´æ–°
}
```

## ğŸ”€ è¿è¡Œæ—¶ Provider åˆ‡æ¢

### çƒ­é‡è½½æœºåˆ¶

**API ç«¯ç‚¹**: `POST /api/config/switch-provider`

**æ–‡ä»¶**: `api/config/mod.rs`

```rust
pub async fn switch_provider(
    State(state): State<AppState>,
    Json(request): Json<SwitchProviderRequest>,
) -> Result<Json<serde_json::Value>, StatusCode> {
    // 1. éªŒè¯ provider
    validate_provider(&request.provider)?;
    
    // 2. æ„å»ºæ–°çš„ backend settings
    let new_backend = match request.provider.as_str() {
        "minimax" => LlmBackendSettings::Minimax {
            api_key,
            model,
        },
        // ... å…¶ä»– providers
    };
    
    // 3. æ›´æ–°æœåŠ¡ï¼ˆæ— éœ€é‡å¯ï¼‰
    state.update_llm_service(&new_backend)?;
    
    Ok(Json(json!({
        "status": "success",
        "provider": request.provider,
        "restart_required": false,
    })))
}
```

**AppState::update_llm_service**:
```rust
pub fn update_llm_service(&self, new_backend: &LlmBackendSettings) -> Result<()> {
    // åˆ›å»ºæ–°çš„ LLM æœåŠ¡
    let new_service = LlmService::new(new_backend)?;
    
    // æ›´æ–°æœåŠ¡ï¼ˆåŸå­æ“ä½œï¼‰
    {
        let mut service = self.llm_service.write()?;
        *service = new_service;
    }
    
    // æ›´æ–°é…ç½®
    {
        let mut config = self.config.write()?;
        config.llm_backend = new_backend.clone();
    }
    
    Ok(())
}
```

## ğŸ“Š Provider é…ç½®æ˜ å°„

### Provider åç§° â†’ ç¯å¢ƒå˜é‡

| Provider | ç¯å¢ƒå˜é‡ | é»˜è®¤æ¨¡å‹ | API ç±»å‹ |
|----------|----------|----------|----------|
| minimax | `MINIMAX_API_KEY` | `MiniMax-M2` | OpenAI Compatible |
| openai | `OPENAI_API_KEY` | `gpt-4` | Native |
| anthropic | `ANTHROPIC_API_KEY` | `claude-3-5-sonnet-20241022` | Native |
| ollama | (æ— ) | `llama2` | Native |
| zhipu | `ZHIPU_API_KEY` | `glm-4-flash` | OpenAI Compatible |
| moonshot | `MOONSHOT_API_KEY` | `kimi-k2-turbo-preview` | OpenAI Compatible |
| longcat | `LONGCAT_API_KEY` | `LongCat-Flash-Chat` | OpenAI Compatible |
| aliyun | `ALIYUN_API_KEY` | `qwen-max` | Native |
| volcengine | `VOLCENGINE_API_KEY` | `doubao-pro-32k` | Native |
| tencent | `TENCENT_API_KEY` | `hunyuan-lite` | Native |

## ğŸ¯ æ·»åŠ æ–° Provider çš„æ­¥éª¤

### 1. åœ¨ `settings.rs` æ·»åŠ æšä¸¾å˜ä½“

```rust
pub enum LlmBackendSettings {
    // ... ç°æœ‰ providers
    NewProvider {
        api_key: String,
        model: String,
    },
}
```

### 2. åœ¨ `llm/mod.rs` æ·»åŠ å®¢æˆ·ç«¯åˆ›å»ºé€»è¾‘

```rust
LlmBackendSettings::NewProvider { api_key, .. } => {
    LlmClient::new_provider(api_key)?
},
```

### 3. åœ¨ `cli/loader.rs` æ·»åŠ é…ç½®å¤„ç†

```rust
// ç¯å¢ƒå˜é‡æ˜ å°„
"newprovider" => std::env::var("NEW_PROVIDER_API_KEY").ok(),

// é»˜è®¤æ¨¡å‹
"newprovider" => "default-model".to_string(),

// Backend è®¾ç½®åˆ›å»º
"newprovider" => LlmBackendSettings::NewProvider {
    api_key: api_key_value,
    model: model_name,
},
```

### 4. åœ¨æ‰€æœ‰ match è¯­å¥ä¸­æ·»åŠ åˆ†æ”¯

éœ€è¦æ›´æ–°çš„æ–‡ä»¶ï¼š
- `settings.rs` - `get_model()` æ–¹æ³•
- `service.rs` - `Service::new()`
- `llm/models.rs` - provider åç§°æ˜ å°„
- `api/mod.rs` - provider åç§°è·å–
- `api/config/mod.rs` - æ‰€æœ‰é…ç½®ç›¸å…³å‡½æ•°
- `api/openai.rs`, `ollama.rs`, `anthropic.rs` - provider è¯†åˆ«

### 5. åœ¨ `models.yaml` æ·»åŠ æ¨¡å‹é…ç½®

```yaml
newprovider:
  models:
    - id: "model-1"
      name: "Model 1"
      description: "Description"
```

## ğŸ” å…³é”®è®¾è®¡æ¨¡å¼

### 1. æšä¸¾åˆ†å‘ï¼ˆEnum Dispatchï¼‰

ä½¿ç”¨ Rust æšä¸¾è¿›è¡Œç±»å‹åˆ†å‘ï¼š
```rust
match config {
    LlmBackendSettings::Minimax { .. } => { /* Minimax é€»è¾‘ */ },
    LlmBackendSettings::OpenAI { .. } => { /* OpenAI é€»è¾‘ */ },
    // ...
}
```

### 2. çº¿ç¨‹å®‰å…¨çš„çŠ¶æ€ç®¡ç†

ä½¿ç”¨ `Arc<RwLock<>>` å®ç°å¹¶å‘è®¿é—®ï¼š
```rust
pub struct AppState {
    pub llm_service: Arc<RwLock<LlmService>>,
    pub config: Arc<RwLock<Settings>>,
}
```

### 3. é…ç½®çƒ­é‡è½½

æ— éœ€é‡å¯å³å¯åˆ‡æ¢ Providerï¼š
```rust
state.update_llm_service(&new_backend)?;
```

### 4. ç»Ÿä¸€æ¥å£æŠ½è±¡

æ‰€æœ‰ Provider é€šè¿‡ `LlmClient` ç»Ÿä¸€æ¥å£ï¼š
```rust
pub async fn chat(&self, model: &str, messages: Vec<Message>) -> Result<Response>;
pub async fn chat_stream(&self, ...) -> Result<Stream>;
```

## ğŸ“ æ€»ç»“

**Provider è®¾ç½®çš„æ ¸å¿ƒæµç¨‹**:

1. **å‘½ä»¤è¡Œè§£æ** â†’ è·å– `--provider` å‚æ•°
2. **é…ç½®åŠ è½½** â†’ ä»ç¯å¢ƒå˜é‡è¯»å– API Keyï¼Œè®¾ç½®é»˜è®¤æ¨¡å‹
3. **ç±»å‹åˆ›å»º** â†’ åˆ›å»º `LlmBackendSettings::Minimax { ... }`
4. **å®¢æˆ·ç«¯åˆå§‹åŒ–** â†’ æ ¹æ® Provider ç±»å‹åˆ›å»ºå¯¹åº”çš„ `LlmClient`
5. **æœåŠ¡åˆ›å»º** â†’ åŒ…è£…ä¸º `Service` å±‚
6. **çŠ¶æ€ç®¡ç†** â†’ å­˜å‚¨åœ¨ `AppState` ä¸­ï¼Œæ”¯æŒçƒ­é‡è½½

è¿™ç§è®¾è®¡ä½¿å¾—æ·»åŠ æ–° Provider åªéœ€è¦ï¼š
- æ·»åŠ æšä¸¾å˜ä½“
- å®ç°å®¢æˆ·ç«¯åˆ›å»ºé€»è¾‘
- æ›´æ–°æ‰€æœ‰ match è¯­å¥
- æ·»åŠ æ¨¡å‹é…ç½®

æ‰€æœ‰ Provider å…±äº«ç›¸åŒçš„æ¥å£å’Œè°ƒç”¨æµç¨‹ï¼Œç¡®ä¿äº†ä»£ç çš„ä¸€è‡´æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚

