# LLM Link Configuration - Aliyun Wanxiang GLM Backend with Ollama Protocol
#
# This configuration allows applications expecting Ollama protocol
# to work with GLM models through Aliyun Wanxiang (OpenAI-compatible API).

# Server configuration
server:
  host: "127.0.0.1"
  port: 8080
  log_level: "info"

# LLM backend configuration - Use Aliyun Wanxiang GLM as backend
llm_backend:
  type: "Aliyun"  # Aliyun Wanxiang API with OpenAI-compatible format
  api_key: "your-aliyun-wanxiang-api-key-here"  # Get this from https://dashscope.console.aliyun.com/
  model: "glm-4.6"  # Available models: glm-4.6, glm-4, glm-4-plus, glm-4-flash, glm-4-air, glm-4-long

# API endpoint configurations
# Enable only Ollama protocol interface
apis:
  # Ollama-compatible API (for applications expecting Ollama)
  ollama:
    enabled: true
    path: "/ollama"
    # Applications will use these endpoints:
    # - POST http://localhost:8080/ollama/api/generate
    # - POST http://localhost:8080/ollama/api/chat
    # - GET  http://localhost:8080/ollama/api/tags
    # - GET  http://localhost:8080/ollama/api/show/:model

  # Disable other APIs if you only need Ollama protocol
  openai:
    enabled: false
    path: "/v1"

  anthropic:
    enabled: false
    path: "/anthropic"

# Example usage:
# 1. Replace "your-aliyun-wanxiang-api-key-here" with your actual Aliyun Wanxiang API key
# 2. Start the service: `./target/release/llm-link -c config-glm-ollama.yaml`
# 3. Your applications can now use Ollama protocol at http://localhost:8080/ollama
# 4. All requests will be translated from Ollama format to Aliyun Wanxiang (OpenAI-compatible) API format

# Test with curl:
# curl -X POST http://localhost:8080/ollama/api/chat \
#   -H "Content-Type: application/json" \
#   -d '{
#     "model": "glm-4.6",
#     "messages": [
#       {"role": "user", "content": "Hello, world!"}
#     ]
#   }'

# API Flow:
# Ollama Request → LLM Link → Aliyun Wanxiang (OpenAI-compatible) → GLM Model