# LLM Link Configuration File
# This file configures the LLM proxy service

# Server configuration
server:
  host: "localhost"
  port: 11434
  log_level: "info"

# LLM backend configuration
# Choose one of the following backends:
llm_backend:
  # Option 1: OpenAI backend
  type: "OpenAI"
  api_key: "ak_11o3bI6O03mx2yS8jb2hD61q7DJ4d"
  base_url: "https://api.openai.com/v1" # Optional, defaults to OpenAI official
  model: "gpt-3.5-turbo"

  # Option 2: Anthropic backend
  # type: "Anthropic"
  # api_key: "your-anthropic-api-key-here"
  # model: "claude-3-sonnet-20240229"

  # Option 3: Ollama backend
  # type: "Ollama"
  # base_url: "http://localhost:11434"  # Optional, defaults to localhost:11434
  # model: "llama2"

  # Option 4: Aliyun backend
  # type: "Aliyun"
  # api_key: "your-aliyun-api-key-here"
  # model: "qwen-turbo"

# API endpoint configurations
# Enable/disable specific API interfaces
apis:
  # Ollama-compatible API
  ollama:
    enabled: true
    path: "/ollama"
    # This will be available at:
    # - POST /ollama/api/generate
    # - POST /ollama/api/chat
    # - GET /ollama/api/tags
    # - GET /ollama/api/show/:model

  # OpenAI-compatible API
  openai:
    enabled: true
    path: "/v1"
    api_key_header: null # Optional custom header name for API key
    # This will be available at:
    # - POST /v1/chat/completions
    # - GET /v1/models

  # Anthropic-compatible API
  anthropic:
    enabled: true
    path: "/anthropic"
    api_key_header: null # Optional custom header name for API key
    # This will be available at:
    # - POST /anthropic/v1/messages
    # - GET /anthropic/v1/models
# Example usage:
# 1. Start the service: `cargo run -- -c llm-link.yaml`
# 2. Or use environment variables:
#    - LLM_LINK_CONFIG=./llm-link.yaml
#    - LLM_LINK_HOST=0.0.0.0
#    - LLM_LINK_PORT=8080
#    - LLM_LINK_LOG_LEVEL=debug
